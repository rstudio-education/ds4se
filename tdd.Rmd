# Does It Really Matter to Test First or Test After?

```{r setup, include=FALSE}
source("etc/common.R")
```

In my experience, the world of software is an *opinionated* one. It seems like everyone has special tools, techniques, practices, and packages that they swear by. In this lesson, we will investigate one of these classic claims: Which is better? Test-first or test-last development? We will also explore what it looks like when people misattribute benefits to a specific practice, when in reality the cause is entirely different.
The study we are using is: [*A Dissection of the Test-Driven Development Process: Does It Really Matter to Test-First or to Test-Last?*](https://arxiv.org/pdf/1611.05994.pdf)[@Fucc2016]
Below is an excellent quote from the paper, highlighting how statistics can separate the relevant findings from other distracting factors:

> This information can liberate developers and organizations
> who are interested in adopting TDD, and trainers who
> teach it, from process dogma based on pure intuition,
> allowing them to focus on aspects that matter most in
> terms of bottom line.

!["Design Thinking" model emphasizes the iterative dynamic of creating something useful](static/design2.png)


## Test-Driven Development Process

- what is it?
- why do people care?

## The Study

- beyond their very complex design
- granularity, uniformity, sequencing, and refactoring
- test-first, test-last

## Modeling Recipe

> Here is a little recipe for [**modeling**](glossary.html#model):

1. Define the [**outcome variable(s)**](glossary.html#dependentvariable) that you care about

2. Define the [**factors**](glossary.html#factor) that might affect (1); be willing to iterate!

3. Determine which kind of model can explain the data we see: [`lm`,`glm`,`chisq`,`anova`](glossary.html#modeltypes) etc. 

4. Keep it as simple as possible, and try to include as few [**features**](glossary.html#factor) and [**interactions**](glossary.html#interaction) as you can. *simpler is better, without [**underfitting**](glossary.html#underfit)*

5. Determine an appropriate metric to [**evaluate the model**](glossary.html#rmse): `rmse`, `R^2`, `AIC`, `BIC`, etc.

6. Compare models to find the model with a nice balance between simplicity and [**goodness of fit**](glossary.html#goodnessoffit).

7. Continue to ask yourself; does this model actually make sense in the world? Even if it mathematically [**fits**](glossary.html#modelfit) really well, is it plausible?

8. If so, then you have a very reasonable model! If you can, test it on new data, or perform an experiment that confirms your [**hypothesis**](glossary.html#hypothesistesting).

9. Without experimentation, you cannot know anything about [**causality**](glossary.html#causality). Instead, your model provides a representation of the phenomenon and the factors involved.



## Outcomes We Care About

> *Which subset of the factors best explain
the variability in external quality?*

So basically this question is asking "what combo of variables contributes to making stuff good?"

With any measure, we have to [**operationalize**](glossary.html#operationalize) it in some way. "Code Quality" is defined as: 
![](static/qlty.PNG), with *QLTYi* defined as: ![](static/qltyi.PNG)

Don't panic. I've actually wanted to do a research study where I use eyetracking technology to watch how learners react to equations in an academic paper. If you're anything like me, your eyes *glaze right over it* and then you curse yourself for not immediately understanding it by just absorbing it into your mind without reading it. When you dig into these equations, you see that it's a fancy way of counting stuff. `TUS` is the number of "Tackled User Stories", or "how many problems attempted". So, for each of those stories, count up the proportion of passing assert statements, and take the average over all the stories attempted. This way, we are simply measuring functional correctedness, with no accounting for things like style or readability.


> *Which subset of the factors best explain
the variability in developer productivity?*

And this question is asking "what combo of variables contributes to developers producing more?" (though you may remember from [Analyze This!](../analyze_this/index.html) that this may be unwise to measure!)

Productivity is defined as: ![](static/prod.png) with OUTPUT as the total passing assert statements. TIME measures from the time the task is opened until closed.


## Factors We Measure

The following is taken directly from Table 2.
 
- **Granularity**: A fine-grained development process is characterized by a cycle duration typically between 5 and 10 minutes. A small value indicates a granular process. A large value indicates a coarse process.

- **Uniformity**: A uniform development process is characterized by cycles having approximately the same duration. A value close to zero indicates a uniform process. A large value indicates a heterogeneous, or unsteady, process.

- **Sequencing**: Indicates the prevalence of test-first sequencing during the development process. A value close to 100 indicates the use of a predominantly test-first dynamic. A value close to zero indicates a persistent violation of thre test-first dynamic.

- **Refactoring**: Indicates the prevalence of the 
refactoring activity in the development process. A value close to zero indicates nearly no detectable refactoring activity (negligable refactoring effort). A value close to 100 indicates a process dominated by refactoring activity (high refactoring effort).

```{r}
data <- read.csv("data/dissectionTDD/dataset.csv",sep=";")
head(data)
```

## Descriptive Statistics
There is a difference between [**descriptive statistics**](glossary.html#descriptivestatistics) and [**inferential statistics**](glossary.html#inferentialstatistics). Descriptive statistics describe properties of the data; such as [**means**](glossary.html#mean), [**ranges**](glossary.html#range), and [**normality**](glossary.html#normaldistribution) of the variables of interest. Inferential statistics will draw the actual *conclusions* about the data; reporting on [**correlations**](glossary.html#correlation), [**hypothesis tests**](glossary.html#hypothesistesting), and [**estimation of parameters]**](glossary.html#parameter). Inferential statistics helps to generalize about a larger [**population**](glossary.html#population), that can go beyond the descriptive statistics of an immediate [**sample**](glossary.html#sample).

```{r}
library(ggplot2)

#small function to generate colors for ggplot
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

titles <- c("Quality","Productivity","Granularity","Uniformity","Sequencing","Refactoring")
#get some colors for each
cols = gg_color_hue(length(titles))

# loop to create 6 density plots to look at spread for each variable
loop <- 2:7
for( i in loop){
  x <- data[[i]]
  plt <- ggplot(data,aes(x)) + 
    ggtitle(paste("Histogram and Density for",titles[i-1]))+
  geom_histogram(aes(y = ..density..), bins=25,color="black",fill=cols[i-1])+
  geom_density(aes(y = ..density..),color="black",fill="black", alpha=.2,stat = 'density')+
    xlab(titles[i-1])+
  theme_bw()
  
  print(plt)
  print(shapiro.test(x))
}
```

## Let's Talk About Models
Remember that we are looking for *which features affect quality and productivity*. Like many model setups, we have collected data and are looking for which features carry the most [**weight**](glossary.html#weight). That means, there are a bunch of factors affecting an outcome (like Productivity) and we want to know which ones matter the most. You might also simply be looking to record the weights on each of the features, not caring which ones "matter" but just trying to get an accurate representation of the phenomenon (say, what is affecting climate change, or what factors lead to higher rates of cancer?) When creating a model, we are looking to represent the various dynamics in the world that make something happen; *we represent it so that we can understand it and predict it*. In our scenario, we have quantitative [**measures**](glossary.html#measure) for the following: **Granularity, Uniformity, Sequencing, Refactoring, Quality**, and **Productivity**. We want to know how `GRA`, `UNI`, `SEQ`, and `REF` are affecting our outcome variables: `PRO` and `QLTY`. Ideally, our objective would be to provide a conclusion like *"The more refactoring you do, the better your code quality, and test-first is better for productivity"* (not an actual conclusion of this paper). This paper keeps a certain tone throughout, emphasizing how we can use actual metrics to stop petty debates within the software world.

If you jump ahead to the **Discussion**, we know that the eventual findings are:

> We conclude that granularity, uniformity and refactoring effort together constitute
the best explanatory factors for external quality [and] productivity.

So what would that conclusion look like in model form? First off, the conclusion implies that `SEQ` does not matter, and is not even included in the best performing model. This is interesting, because so many people evangelize the Test-Driven Development technique! In fact, it's not the sequencing of when you write the tests, but the level to which you iterate rapidly when writing code that affects quality and productivity. It could be the case that people attributed their success to the wrong factor: *test-first* or *test-last*, when in fact, iteration was key to success. 




## Regression Analysis vs. Hypothesis Testing
- explain how because theyre all continuous variables, we dont do hypothesis testing. we use regression
TODO


```{r}

m <- glm(QLTY ~ GRA + UNI + REF + SEQ +GRA:UNI+SEQ:REF, data=data)
m
summary(m)
AIC(m)
summary(lm(QLTY ~ GRA + UNI + REF + SEQ +GRA:UNI+SEQ:REF, data=data))$r.squared


m <- glm(QLTY ~ GRA + UNI +REF, data=data)
#m
#summary(m)
AIC(m)
summary(lm(QLTY ~ GRA + UNI + REF + SEQ, data=data))$r.squared

```

## Let's Talk About Interactions
This paper mentions something called [**interactions**](glossary.html#interaction). We will walk through a more basic example of what an interaction is before exploring what it means in this context. You can imagine a scenario where there is a [**categorical variable**](glossary.html#factor): `test_first`, which can take on either 1 or 0 if the developer tested first or last, respectively. Now imagine we also record whether developers are coding in the morning or the afternoon. *It just so happens in our fake scenario*, that if you test-first in the mornings, your quality skyrockets. If you test-first any other time, you don't get the same effect. Therefore, there is an interaction between testing first and the time of day. This is **problematic** because if we don't look into this interaction, we simply report a mean quality of somewhere in the middle; not accounting for how  the test sequencing and time of day is affecting that quality. **If there is an interaction, the lines in the categorical interaction plot will cross**.

```{r}
fake <- read.csv("data/fake_interaction.csv")
head(fake,10)
fake$morning <- as.factor(fake$morning)
fake$test_first <- as.factor(fake$test_first)
interaction.plot(factor(fake$test_first),factor(fake$morning),fake$QLTY)

```

Take a look at the means when we *do* take into account the interaction, looking at both `test_first` and `morning` as contributing to `QLTY`. If we *only* look at one or the other, our means are misleading, because clearly there is another factor affecting the code quality! Sure, we could say that on average the code quality is somewhere in the middle; but it's disingenuous because the truth is that your code quality will suffer if you're using test_first in the afternoon. (In this **FAKE DATA**, of course). In order to truly represent the phenomenon, we have to include results about the interaction.

```{r}

fake %>%
  group_by(test_first,morning) %>%
  summarise(mean=mean(QLTY))

fake %>%
  group_by(test_first) %>%
  summarise(mean=mean(QLTY))

fake %>%
  group_by(morning) %>%
  summarise(mean=mean(QLTY))

```

## Component + Residual Plots
```{r fig.height=6.5, fig.width=9.5}
library(car)

m<-glm(QLTY~GRA+UNI+REF,data=data)
crPlots(m)

```

<center>![](static/codeworks.jpg)</center><br>

## Interpreting Regression Output
Here I've used a [**general linear model**](glossary.html#modeltypes) to map out the factors affecting code quality. In the paper, they systematically included or excluded different factors and their interactions, in order to find the best fitting model. "Finding the best fitting model" means finding which combination of factors, and in what weights, best represents the relationship between those factors and the outcome we care about (quality or productivity). Remember, you could always add more and more factors to a model; the code will still output an answer. But it doesn't mean it's a *correct* model, or a *useful* one.

Looking at the final model used in the paper, we see that the factors kept were **Granularity, Uniformity, and Refactoring** in order to model **Code Quality**. This is particuarly interesting because **Sequencing** is *not* a factor in the model. The big question from software engineers was about *test-first* vs. *test-last*, but it turns out that it really doesn't matter. The other factors are more important, and have a *negative relationship* with code quality. That negative relationship is because of how the measures were formulated; the closer to 0, the more granular and uniform the cycles were. So, the smaller the values for `GRA`, `UNI` and `REF`, the higher the value on `QLTY`. By looking at the output of the summary of the model, we see the Estimates as negative, and significant (or at least marginally significant). You can see significance by noting the number of stars (***) by each factor. (.) indicates a marginal trend. The t value is also indicative of how large the effect is, with the absolute value of t indicating significance. All of those significance factors are basically saying "hey! these factors are contributing to this outcome in a meaningful way!"

```{r}
summary(m)

```

## Evaluating How Good a Model Is (AIC)
How do we determine which model is the "right" one? Well, there's a few ways to do that. Sometimes, we simply look at how good the model is at predicting test data. In this case, we are using something called AIC (Akaike Information Criterion). This is a very specific way of evaluating a model that deals with *preserving information*. It's interesting and cool, and strikes a nice balance between [**goodness of fit**](glossary.html#goodnessoffit) and *simplicity* (not throwing too many parameters into the model). It's okay if that doesn't make sense; just know that a lower AIC indicates a "better" model. Notice how if we include `SEQ` in the model, the AIC is higher.

```{r}
print(AIC(m))

new_m <- glm(QLTY~GRA+UNI+REF+SEQ,data=data)
print(AIC(new_m))
```

<center>![](static/curvefit.png)</center><br>

### You could try any combination of factors and their interactions, until you achieved the best fitting model for the phenomenon. Davide Fucci carefully walks through model choice, feature selection, investigation of interactions, and more. Follow the paper step by step to replicate their results on this data!

```{r links, child="etc/links.md"}
```
